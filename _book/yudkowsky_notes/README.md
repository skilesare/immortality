# Yudkowsky Notes


- Our examples need to be 'Bayes Optimal'


> Reality, we have learned to our shock, is not a collection of separate magisteria, but a single unified process governed by mathematically simple low-level rules.


Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 13327-13328). Machine Intelligence Research Institute. Kindle Edition.

> So unless you can tell me which specific step in your argument violates the laws of physics by giving you true knowledge of the unseen, don’t expect me to believe that a big, elaborate clever argument can do it either.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 13485-13486). Machine Intelligence Research Institute. Kindle Edition.

> Unanswerable questions do not mark places where magic enters the universe. They mark places where your mind runs skew to reality.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 13755-13756). Machine Intelligence Research Institute. Kindle Edition.

> Surprise exists in the map, not in the territory.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Location 14086). Machine Intelligence Research Institute. Kindle Edition.

> This, as I see it, is the thesis of reductionism. Reductionism is not a positive belief, but rather, a disbelief that the higher levels of simplified multilevel models are out there in the territory. Understanding this on a gut level dissolves the question of “How can you say the airplane doesn’t really have wings, when I can see the wings right there?” The critical words are really and see.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 14190-14193). Machine Intelligence Research Institute. Kindle Edition.

> It takes a strong mind, a deep honesty, and a deliberate effort to say, at this point, “That which can be destroyed by the truth should be,” and “The scientist hasn’t taken the gnomes away, only taken my delusion away,”

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 14262-14263). Machine Intelligence Research Institute. Kindle Edition.

> They are savanna poets, who can only tell stories that would have made sense around a campfire ten thousand years ago. Savanna poets, who can tell only the Great Stories in their classic forms, and nothing more.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 14391-14392). Machine Intelligence Research Institute. Kindle Edition.

> At the very least, to get close to the truth, you have to be willing to press yourself up against reality as tightly as possible, without flinching away, or sneering down.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 14521-14522). Machine Intelligence Research Institute. Kindle Edition.

> But if you don’t have the capacity to enjoy hang-gliding even though it is merely real, then as soon as dragons turn real, you’re not going to be any more excited by dragons than you are by hang-gliding.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 14582-14583). Machine Intelligence Research Institute. Kindle Edition.

"And this is why you are going to invest your money with me.  I'm wiling to bind my self to reality where you are not.  I'm will to push churches, states, families to a place to a place where they will use the merely real as magic to change the course of history.  You could think for a life time about all the magical things that you want to do with your money.  You will never accomplich a single magical act.  I will accomplish the real.  I'm bound to it." - AAF

> So remember the Litany Against Being Transported Into An Alternate Universe: If I’m going to be happy anywhere, Or achieve greatness anywhere, Or learn true secrets anywhere, Or save the world anywhere, Or feel strongly anywhere, Or help people anywhere, I may as well do it in reality.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 14611-14616). Machine Intelligence Research Institute. Kindle Edition.

> The Ultimate Power. The user of this ability contains a smaller, imperfect echo of the entire universe, enabling them to search out paths through probability to any desired future. If this sounds like a ridiculously powerful ability, you’re right— game balance goes right out the window with this one. Extremely rare among life forms, it is the sekai no ougi or “hidden technique of the world.”

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 14654-14657). Machine Intelligence Research Institute. Kindle Edition.

> If God did speak plainly, and answer prayers reliably, God would just become one more boringly real thing, no more worth believing in than the postman.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 14792-14793). Machine Intelligence Research Institute. Kindle Edition.

"So is he witholding the answer an the action to be tricksy?  Is that the God you think you worship?  The Tricksy one?" -AAF

> There is nothing but science to learn— the emotional energy must either be invested in reality, or wasted in total nonsense, or destroyed. For myself, I think it is better to invest the emotional energy; fun should not be needlessly cast away.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 14993-14995). Machine Intelligence Research Institute. Kindle Edition.

> The map is multilevel, the territory is single-level. This doesn’t mean that the higher levels “don’t exist,”
... Implicit existence is not the same as nonexistence. The exact description of this implicitness is not known to us— is not explicitly represented in our map. But this does not prevent our map from working, or even prevent it from being true.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 15544-15545). Machine Intelligence Research Institute. Kindle Edition.

> I am speaking semi-metaphorically here— there are things in the universe besides quarks— but the point is, postulating an extra billion galaxies doesn’t count against the size of your code, if you’ve already described one galaxy. It just takes a bit more RAM, and Occam’s Razor doesn’t care about RAM.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 16394-16396). Machine Intelligence Research Institute. Kindle Edition.

> By far the best definition I’ve ever heard of the supernatural is Richard Carrier’s: A “supernatural” explanation appeals to ontologically basic mental things, mental entities that cannot be reduced to nonmental entities.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 16555-16558). Machine Intelligence Research Institute. Kindle Edition.

> Suppose that a 747 had a fundamental physical existence apart from the quarks making up the 747. What experimental observations would you expect to make, if you found yourself in such a universe? If you can’t come up with a good answer to that, it’s not observation that’s ruling out “non-reductionist” beliefs, but a priori logical incoherence. If you can’t say what predictions the “non-reductionist” model makes, how can you say that experimental evidence rules it out?

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 16583-16587). Machine Intelligence Research Institute. Kindle Edition.

###Contrast the above vs. Alexander in future study

> The Jehovah model of our alien god is coherent, falsifiable, and wrong— coherent, that is, so long as you don’t care whether Jehovah is ontologically basic or just an alien.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 16645-16646). Machine Intelligence Research Institute. Kindle Edition.

> But the ultimate rule of science is to look and see. If ever a God appeared to thunder upon the mountains, it would be something that people looked at and saw.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 16655-16656). Machine Intelligence Research Institute. Kindle Edition.

> "Many-worlds wins outright given our current state of evidence. There is no more reason to postulate a single Earth, than there is to postulate that two colliding top quarks would decay in a way that violates Conservation of Energy. It takes more than an unknown fundamental law; it takes magic. The debate should already be over. It should have been over fifty years ago. The state of evidence is too lopsided to justify further argument. There is no balance in this issue.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 18389-18392). Machine Intelligence Research Institute. Kindle Edition.


> “Trained to manipulate whatever science had already been discovered,” said Taji. “It was a difficult enough task for Eld teachers to train their students to use existing knowledge, or follow already-known methodologies; that was all Eld science teachers aspired to impart.”

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Location 18536). Machine Intelligence Research Institute. Kindle Edition.

> You don’t shake your finger at people for being selfish. You try to build an efficient system of production out of selfish participants, by requiring transactions to be voluntary. So people are forced to play positive-sum games, because that’s how they get the other party to sign the contract. With violence restrained and contracts enforced, individual selfishness can power a globally productive system.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 18663-18666). Machine Intelligence Research Institute. Kindle Edition.

> Libertarianism secretly relies on most individuals being prosocial enough to tip at a restaurant they won’t ever visit again. An economy of genuinely selfish human-level agents would implode. Similarly, Science relies on most scientists not committing sins so egregious that they can’t rationalize them away.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 18685-18688). Machine Intelligence Research Institute. Kindle Edition.

> I did not generalize the concept of “mysterious answers to mysterious questions,” in that many words, until I was writing a Bayesian analysis of what distinguishes technical, nontechnical and semitechnical scientific explanations. Now, the final output of that analysis can be phrased nontechnically in terms of four danger signs: First, the explanation acts as a curiosity-stopper rather than an anticipation-controller. Second, the hypothesis has no moving parts— the secret sauce is not a specific complex mechanism, but a blankly solid substance or force. Third, those who proffer the explanation cherish their ignorance; they speak proudly of how the phenomenon defeats ordinary science or is unlike merely mundane phenomena. Fourth, even after the answer is given, the phenomenon is still a mystery and possesses the same quality of wonderful inexplicability that it had at the start.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 18891-18900). Machine Intelligence Research Institute. Kindle Edition.

> It is living with uncertainty— knowing on a gut level that there are flaws,they are serious and you have not found them— that is the difficult thing.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Location 19061). Machine Intelligence Research Institute. Kindle Edition.

> In the previous essay I argued that the Powers Beyond Science are actually a standard and necessary part of the social process of science. In particular, scientists must call upon their powers of individual rationality to decide what ideas to test, in advance of the sort of definite experiments that Science demands to bless an idea as confirmed. The ideal of Science does not try to specify this process— we don’t suppose that any public authority knows how individual scientists should think— but this doesn’t mean the process is unimportant.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 19175-19177). Machine Intelligence Research Institute. Kindle Edition.

The above is a great statement about why we need incetivise the forward looking ability of society so that we 'guess better'....or rather 'pick the best guess' more often. -AAF

> evidence. But Einstein, by looking at other laws, cut down the space of possibilities for the next law. He learned the alphabet in which physics was written, constraints to govern his answer. Not magic, but reasoning on a higher level, across a wider domain, than what a naive reasoner might conceive to be the “model space” of only this one law.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 19247-19250). Machine Intelligence Research Institute. Kindle Edition.

> But sometimes experiments are costly, and sometimes we prefer to get there first . . . so you might consider trying to train yourself in reasoning on scanty evidence, preferably in cases where you will later find out if you were right or wrong. Trying to beat low-capitalization prediction markets might make for good training in this?— though that is only speculation.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 19289-19292). Machine Intelligence Research Institute. Kindle Edition.

Low capitalization prediction markets sounds interesting. Explore further - AAF TODO

> The logarithm scoring rule is proper— it has its expected maximum when we say our exact anticipations; it rewards honesty. If we think the blue light has a 60% probability of flashing, and we calculate our expected payoff for different betting schemas, we find that we maximize our expected payoff by telling the experimenter “60%.”

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 19851-19854). Machine Intelligence Research Institute. Kindle Edition.

Try to apply this log rule to payouts for  pref payments - AAF TODO

> Your calibration is perfect and mine isn’t, but my better discrimination between right and wrong answers more than makes up for it. My final score is higher— I assigned a greater joint probability to the final outcome of the entire experiment. If I’d been less overconfident and better calibrated, the probability I assigned to the final outcome would have been 0.88 × 0.22, which works out to 0.006 or 6%.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 19896-19899). Machine Intelligence Research Institute. Kindle Edition.

Also need to take into account this discrimination. -AAF

> “Whereas good calibration often can be achieved by simple mathematical transformations (e.g., adding a constant to every probability judgment), good discrimination demands access to solid, predictive evidence and skill at exploiting that evidence, which are difficult to find in any real-life, practical situation.” If you lack the ability to distinguish truth from falsehood, you can achieve perfect calibration by confessing your ignorance; but confessing ignorance will not, of itself, distinguish truth from falsehood.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 19911-19914). Machine Intelligence Research Institute. Kindle Edition.

> Our scoring rule encourages you to do better if you can. If you are ignorant, confess your ignorance; if you are confident, confess your confidence. We penalize you for being confident and wrong, but we also reward you for being confident and right. That is the virtue of a proper scoring rule.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 19916-19918). Machine Intelligence Research Institute. Kindle Edition.

> Why did the vague theory lose when both theories fit the evidence? The vague theory is timid; it makes a broad prediction, hedges its bets, allows many possibilities that would falsify the precise theory.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 19959-19961). Machine Intelligence Research Institute. Kindle Edition.

Maybe making a broad bet on the economy will lose to a specific bet.  Is the 'market' vague? Maybe this is exactly the innovation...make the broad bet 0 sum and still offer an advantage to being specific.-AAF

> Precision or vagueness is a property of maps, not territories. Rather we should ask if the price in the supermarket stays constant or shifts about. A hypothesis of the “vague” sort is a good description of a price that shifts about. A precise map will suit a constant territory.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 19998-19999). Machine Intelligence Research Institute. Kindle Edition.

> When I judge the predictions of a hypothesis, I ask which experiences I would anticipate, not which facts I would believe.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 20284-20285). Machine Intelligence Research Institute. Kindle Edition.


On to morrality:

> You shouldn’t expect to be able to compress a human morality down to a simple utility function, any more than you should expect to compress a large computer file down to 10 bits.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 20896-20897). Machine Intelligence Research Institute. Kindle Edition.


> Or to phrase that somewhat less paradoxically: We should look for valuations that are over 4D states, rather than 3D states. Valuable ongoing processes, rather than “make the universe have property P and then you’re done.”

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 22288-22289). Machine Intelligence Research Institute. Kindle Edition.

> One answer might be that what a story really needs is not “disaster,” or “pain,” or even “conflict,” but simply striving. That the problem with Mary Sue stories is that there’s not enough striving in them, but they wouldn’t actually need pain. This might, perhaps, be tested.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 22384-22386). Machine Intelligence Research Institute. Kindle Edition.

The anti-fragility of hypercatallaxy allows us to over reach and have a safety blanket. Reach for x+2 with a cheering crowd. If you fall short and only hit x+1, or worse end up at x-3, the network absorbs the pain.  On to the next adventure. -AAF

> How did evolution, which is itself so uncaring, create minds on that qualitatively higher moral level? How did evolution, which is so ugly, end up doing anything so beautiful?

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 22672-22673). Machine Intelligence Research Institute. Kindle Edition.

> Most people choose option 1. Which, I think, is foolish; because if you multiply 500 lives by 90% probability, you get an expected value of 450 lives, which exceeds the 400-life value of option 1. (Lives saved don’t diminish in marginal utility, so this is an appropriate calculation.)

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 23014-23016). Machine Intelligence Research Institute. Kindle Edition.

Being saved from the downside allows us to take the risk for the upside without fear of 'losing it all'.  This is harder to think about with human lives, but we need to. -AAF

> I would rather see music composed by people than by nonsentient machine learning algorithms, so that someone should have the joy of composition; I care about the journey, as well as the destination.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 23182-23183). Machine Intelligence Research Institute. Kindle Edition.

> It is more important that lives be saved, than that we conform to any particular ritual in saving them. And the optimal path to that destination is governed by laws that are simple, because they are math. And that’s why I’m a utilitarian— at least when I am doing something that is overwhelmingly more important than my own feelings about it— which is most of the time, because there are not many utilitarians, and many things left undone.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 23189-23193). Machine Intelligence Research Institute. Kindle Edition.

> Curiosity, relinquishment, lightness, evenness, argument, empiricism, simplicity, humility, perfectionism, precision, scholarship, and the void.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 23885-23886). Machine Intelligence Research Institute. Kindle Edition.

Integrate Alexander's 15 properties across rationality.  Do they line up with the 12 virtues mentioned here. - AAF

> (I consider it drop-dead obvious that the task of verifying acquired skills and hence the power to grant degrees should be separated from the institutions that do the teaching, but let’s not go into that.)

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 26108-26109). Machine Intelligence Research Institute. Kindle Edition.

Interesting to think of in terms of Mindwallet. - AAF

> “That which can be destroyed by the truth should be.” We should aspire to feel the emotions that fit the facts, not aspire to feel no emotion.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 26260-26262). Machine Intelligence Research Institute. Kindle Edition.

> You’ve got to teach both sides of it, “That which can be destroyed by the truth should be,” and “That which the truth nourishes should thrive.”

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 26265-26266). Machine Intelligence Research Institute. Kindle Edition.

" And now I'll take questions, but there is a catch, you must first announce what amount you will be donating to the kickstarter.  It can be $1 or thousands(don't forget to do this).  If you disagree with what I'm saying, surely it is worth at least a dollar to have your concern heard." - AAF

> But it seems to me that a reasonable rule of thumb might be as follows: If, on the whole, joining your efforts to a group project would still have a net positive effect according to your utility function—(or a larger positive effect than any other marginal use to which you could otherwise put those resources, although this latter mode of thinking seems little-used and humanly-unrealistic, for reasons I may write about some other time) —and the awful horrible annoying issue is not so important that you personally will get involved deeply enough to put in however many hours, weeks, or years may be required to get it fixed up— —then the issue is not worth you withholding your energies from the project; either instinctively until you see that people are paying attention to you and respecting you, or by conscious intent to blackmail the group into getting it done. And if the issue is worth that much to you . . . then by all means, join the group and do whatever it takes to get things fixed up.


Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 26368-26369). Machine Intelligence Research Institute. Kindle Edition.

> Strong communities built around worthwhile purposes: That would be the shape I would like to see for the post-religious age, or whatever fraction of humanity has then gotten so far in their lives.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 26546-26548). Machine Intelligence Research Institute. Kindle Edition.

> But it’s still an interesting point that Science manages to survive not because it is in our collective individual interest to see Science get done, but rather, because Science has fastened itself as a parasite onto the few forms of large organization that can exist in our world.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 26662-26664). Machine Intelligence Research Institute. Kindle Edition.


How do we help science get done?  Are we naive in assuming that increased spending and profits at corporatins would lead to more in the sciences?  Could the Issuing Authority just fund it all and jack up the demurrage? -AAF

> There is this very, very old puzzle/ observation in economics about the lawyer who spends an hour volunteering at the soup kitchen, instead of working an extra hour and donating the money to hire someone to work for five hours at the soup kitchen. There’s this thing called “Ricardo’s Law of Comparative Advantage.” There’s this idea called “professional specialization.” There’s this notion of “economies of scale.” There’s this concept of “gains from trade.” The whole reason why we have money is to realize the tremendous gains possible from each of us doing what we do best.

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 26691-26696). Machine Intelligence Research Institute. Kindle Edition.

> It does seem to me offhand that there ought to be things doable to diminish the pain of losing hit points, and to increase the felt strength of the connection from donating money to “I did a good thing!”

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 26715-26717). Machine Intelligence Research Institute. Kindle Edition.

> But mostly I just hand you an open, unsolved problem: make it possible/ easier for groups of strangers to coalesce into an effective task force over the Internet, in defiance of the usual failure modes

Yudkowsky, Eliezer (2015-03-11). Rationality: From AI to Zombies (Kindle Locations 26893-26894). Machine Intelligence Research Institute. Kindle Edition.
